{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"siamese_face_match.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GTEqqZMIiom-"},"source":["#### Download data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2XSU2sC0zj_q"},"source":["Data for this exercise is available on [this](https://drive.google.com/open?id=0B29vNACcjvzVc1RfVkg5dUh2b1E) Google Drive link. \n","\n","\n","\n","1.  we will uploadour zip file.\n","2.   The database dataset contains the signatures of **5 border**..\n","3.   For each border, there are **4 genuine** and **5 different** border available. \n","4. first 4 are real and next 4 are diffrent borders\n","\n","**Paper reference**: https://arxiv.org/pdf/1707.02131.pdf\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ot8nWnE0H75a"},"source":[""]},{"cell_type":"code","metadata":{"id":"LQHmn-kPcA2s"},"source":["!pip freeze"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dK9OyS0uYUJ2"},"source":["!python --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLOYv28B0ZMm"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hu17NPWGb7yU"},"source":["tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5r7hp0hezsA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Of7fYRkOsMbN"},"source":["%cd '/content/drive/MyDrive/siamese_face_match'\n","%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8wh90Ho6T2x"},"source":["#### Data preparation\n","\n","We will use only border images."]},{"cell_type":"markdown","metadata":{"id":"smlHEXb86oeU"},"source":["Get list of signature directories (1 directory per person)"]},{"cell_type":"markdown","metadata":{"id":"S18S_uQ77kuC"},"source":["Get signature filenames for each person (1 Directory).\n","- For each border, first 4 pictures are orignal border\n","- Last 5 pictures in the directory are different"]},{"cell_type":"code","metadata":{"id":"PbsbvYcILSJM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rT0lgAPCjImF"},"source":["train_dir = '/content/drive/MyDrive/siamese_face_match/data/train'\n","test_dir = '/content/drive/MyDrive/siamese_face_match/data/test'\n","\n","train_images = []\n","test_images = []\n","q = []\n","\n","class_folders_train = os.listdir(train_dir)\n","class_folders_train.sort()\n","total_classes_train = len(class_folders_train)\n","\n","print(\"Trainable Classes Found: \", total_classes_train)\n","print(\"Saving dataset into lists\")\n","\n","for image in class_folders_train:\n","    images = os.listdir(train_dir + \"/\" + image)\n","    images.sort()\n","    images = [train_dir + '/' + image + '/' + x for x in images]\n","    train_images.append(images)\n"," \n","    \n","    \n","    \n","class_folders_test = os.listdir(test_dir)\n","class_folders_test.sort()\n","total_classes_test = len(class_folders_test)\n","\n","for image in class_folders_test:\n","    images = os.listdir(test_dir + \"/\" + image)\n","    images.sort()\n","    images = [test_dir + '/' + image + '/' + x for x in images]\n","    test_images.append(images)\n","\n","print(\"Saved Training and Test dataset\")\n","print(\"Saved Successfully\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhpW5TPkuwr5"},"source":["train_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWzByjVDpbY0"},"source":["test_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KoVk7yzc9V7Z"},"source":["Split between Training and Test. We will use first 80% directories for training and last 20% for test."]},{"cell_type":"code","metadata":{"id":"1bbIKdWz9iLf"},"source":["train_g, test_g = train_images , train_images\n","train_f, test_f = test_images, test_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJ7CheTq979y"},"source":["#### Visualize Signatures"]},{"cell_type":"code","metadata":{"id":"dSxGYc3ZnWE5"},"source":["def visualize_border():\n","\n","    \"\"\"\n","    1. Randomly select a person id\n","    2. Show two genuine signatures for the person\n","    3. Show one forged signature for the same person\n","    \"\"\"\n","    \n","    #Pick up a person from 160 people\n","    person_id = np.random.randint(0, len(train_images))\n","\n","    #Read genuine signature pics\n","    genuine1, genuine2 = np.random.randint(0, 4, 2) #Get two pics randomly\n","    original_img = tf.keras.preprocessing.image.load_img(train_images[person_id][genuine1])#, color_mode='grayscale')\n","    genuine_img = tf.keras.preprocessing.image.load_img(train_images[person_id][genuine2])#, color_mode='grayscale')\n","\n","    #Read forged signature of same person\n","    forged1 = np.random.randint(0, 4)\n","    forged_img = tf.keras.preprocessing.image.load_img(test_images[person_id][forged1])#, color_mode='grayscale')\n","\n","    #Display pictures    \n","    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 10))\n","\n","    ax1.set_title('orignial border')\n","    ax1.imshow(original_img, cmap = 'gray')\n","\n","    ax2.set_title('Another orignial border')\n","    ax2.imshow(genuine_img, cmap = 'gray')\n","    \n","    ax3.set_title('difference border')\n","    ax3.imshow(forged_img, cmap = 'gray')\n","\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODvJ52T3-AMu"},"source":["visualize_border()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YAIYkwwB-5Z"},"source":["#### Building borders pairs"]},{"cell_type":"markdown","metadata":{"id":"YmWAXweiCeRI"},"source":["Siamese network requires **two inputs** (rather than one we use with other models). In this case, the two input could be a \n","1. Combination of **original-original** border\n","2. Combination of **original-difference** border\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SMLXIdNODe0U"},"source":["How many pairs can we create for model training. For each person...\n","\n","1. \n","2.\n","\n","This will make distribution to be *i.e* 1:3 (approximate). \n","\n","T."]},{"cell_type":"markdown","metadata":{"id":"LG06fc0SFE6G"},"source":["**Question: How many pairs we will have in training and test dataset?**"]},{"cell_type":"markdown","metadata":{"id":"3xFThzdqGSRZ"},"source":["Build genuine-genuine pairs"]},{"cell_type":"code","metadata":{"id":"0Cgp5DiTGUWd"},"source":["def build_genuine_pairs(sig_list):\n","\n","    pairs_list = []\n","\n","    for person_id in range(5):\n","\n","        for i in range(len(sig_list[0])-1):\n","            for j in range(i+1, len(sig_list[0])):\n","\n","                pairs_list.append([sig_list[person_id][i], sig_list[person_id][j]])\n","    \n","    return pairs_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EZaAnpNHaWo"},"source":["#Build training and test pairs\n","train_g_g_pairs = build_genuine_pairs(train_g)\n","test_g_g_pairs = build_genuine_pairs(test_g)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnYj19wtHogp"},"source":["#Check number of pairs in training and test\n","print('Number of genuine pairs in training set:', len(train_g_g_pairs))\n","print('Number of genuine pairs in test set:', len(test_g_g_pairs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXAvUc6zID7m"},"source":["Build genuine-fake pairs"]},{"cell_type":"code","metadata":{"id":"T-s1Yr8nIGMA"},"source":["def build_gen_forged_pairs(gen_sigs, forged_sigs):\n","\n","    pairs_list = []\n","\n","    for person_id in range(5):\n","\n","        #Let's pickup 4 random numbers for border \n","        forged_ids = np.random.randint(0, 4,1)\n","\n","        for i in range(4):\n","            for j in range(4):\n","                pairs_list.append([gen_sigs[person_id][i], forged_sigs[person_id][j]])\n","    \n","    return pairs_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FdgxIqjJYof"},"source":["#Build training and test pairs\n","train_g_f_pairs = build_gen_forged_pairs(train_g, train_f)\n","test_g_f_pairs = build_gen_forged_pairs(test_g, test_f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaagN-Z1J_o8"},"source":["#Check number of pairs in training and test\n","print('Number of genuine-forged pairs in training set:', len(train_g_f_pairs))\n","print('Number of genuine-forged pairs in test set:', len(test_g_f_pairs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWT_Wb_OLaBk"},"source":["#### Build Batch Generator"]},{"cell_type":"code","metadata":{"id":"nE5HMf3XZLzw"},"source":["img_width = 300\n","img_height = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ac1ky-FLcV0"},"source":["def batch_generator(gen_gen_list, gen_forged_list, batch_size=32):\n","\n","\n","    while True:\n","\n","        first_img_array = np.zeros((batch_size, img_height, img_width, 3))\n","        second_img_array = np.zeros((batch_size, img_height, img_width, 3))\n","        batch_labels = np.zeros((batch_size, 1))\n","\n","        #Generate batch_size ids for both type of pairs\n","        gen_gen_pair_idx = np.random.randint(0, len(gen_gen_list), batch_size//2)\n","        gen_forged_pair_idx = np.random.randint(0, len(gen_forged_list), batch_size//2)\n","\n","        for i in range(batch_size//2):\n","\n","            #Get images from gen_gen pair\n","            gg_id = gen_gen_pair_idx[i]\n","            first_img = tf.keras.preprocessing.image.load_img(gen_gen_list[gg_id][0], target_size=(img_height, img_width))\n","            second_img = tf.keras.preprocessing.image.load_img(gen_gen_list[gg_id][1], target_size=(img_height, img_width))\n","            \n","            first_img_array[2*i] = tf.keras.preprocessing.image.img_to_array(first_img)\n","            second_img_array[2*i] = tf.keras.preprocessing.image.img_to_array(second_img)\n","\n","            #Genuine genuine pair will be a given a label of '1'\n","            batch_labels[2*i] = 1\n","\n","            #Get images from gen_forged pair\n","            gf_id = gen_forged_pair_idx[i]\n","            first_img = tf.keras.preprocessing.image.load_img(gen_forged_list[gf_id][0], target_size=(img_height, img_width))\n","            second_img = tf.keras.preprocessing.image.load_img(gen_forged_list[gf_id][1], target_size=(img_height, img_width))\n","            \n","            first_img_array[2*i+1] = tf.keras.preprocessing.image.img_to_array(first_img)\n","            second_img_array[2*i+1] = tf.keras.preprocessing.image.img_to_array(second_img)\n","\n","            #Genuine genuine-forged pair will be a given a label of '0'\n","            batch_labels[2*i+1] = 0\n","        \n","        #Normalize data\n","        first_img_array = tf.keras.applications.mobilenet.preprocess_input(first_img_array)\n","        second_img_array = tf.keras.applications.mobilenet.preprocess_input(second_img_array)\n","\n","        yield [first_img_array, second_img_array], batch_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KflfrwaObbTX"},"source":["#Check batch generator\n","a = batch_generator(train_g_g_pairs, train_g_g_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpuYBFRjbivU"},"source":["X, y = next(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFwFZWkQfDmw"},"source":["y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkMXRqyZfSto"},"source":["#### Build Model"]},{"cell_type":"markdown","metadata":{"id":"pix5JrrrgKgF"},"source":["Load a pre-trained model (we can build a model from scratch as well)"]},{"cell_type":"code","metadata":{"id":"vql7Dga6fT_7"},"source":["mobilenet = tf.keras.applications.mobilenet.MobileNet(include_top=False, \n","                                                      input_shape=(img_height, img_width,3),\n","                                                      alpha=0.25,\n","                                                      weights='imagenet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jN94jN-5fuCz"},"source":["mobilenet.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XSccHAlwgVB4"},"source":["Build a Siamese Network using Mobilenet as feature generator"]},{"cell_type":"code","metadata":{"id":"mP7OWcFTgbl1"},"source":["#Create two input layers - first and second image\n","first_input = tf.keras.layers.Input(shape=(img_height, img_width,3))\n","second_input = tf.keras.layers.Input(shape=(img_height, img_width,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mowM7gXb-caK"},"source":[""]},{"cell_type":"code","metadata":{"id":"qexPPs_Ugrrd"},"source":["#Generate features for first and second image\n","first_img_features = mobilenet(first_input)\n","second_img_features = mobilenet(second_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e952as6rgr4A"},"source":["#Size of the outputs\n","first_img_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_gjhK_Qjqmc"},"source":["#Lets flatten the features using Average pooling\n","gap_layer = tf.keras.layers.GlobalAveragePooling2D()\n","\n","#First img features\n","first_img_features = gap_layer(first_img_features)\n","#Second image features\n","second_img_features = gap_layer(second_img_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_XtMvU0kGky"},"source":["first_img_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFIcFINMkLu-"},"source":["We want to calculate Eucledean distance between two feature set. As there is no pre-built Eucledean distance layer in Keras, we will build one."]},{"cell_type":"code","metadata":{"id":"T8yA71drkGvK"},"source":["def euclidean_distance(features):\n","    \n","    #Get features\n","    x, y = features\n","\n","    #Calculate distance\n","    distance = tf.keras.backend.sqrt(tf.keras.backend.sum(tf.keras.backend.square(x - y), axis=1, keepdims=True))\n","    \n","    return distance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Poz-43lWk-3R"},"source":["We will also need a function to define output shape of Eucledean distance layer"]},{"cell_type":"code","metadata":{"id":"wG1XWOH5k40X"},"source":["def eucl_dist_output_shape(shapes):\n","\n","    #Shapes of feature 1 and 2\n","    shape1, shape2 = shapes\n","    \n","    #Returned shape is equal to number of examples, 1\n","    return (shape1[0], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rn5HLsZ-lHP7"},"source":["Use Eucledean distance layer on features"]},{"cell_type":"code","metadata":{"id":"YIOuH3m5lMrM"},"source":["distance = tf.keras.layers.Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([first_img_features, second_img_features])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zT6-P307l5Dc"},"source":["Build model"]},{"cell_type":"code","metadata":{"id":"5-779UAgl3-N"},"source":["model = tf.keras.Model([first_input, second_input], distance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GQD86eEmHkm"},"source":["How do we calculate loss for Siamese network?"]},{"cell_type":"code","metadata":{"id":"YZogiZ3UnWFf"},"source":["def contrastive_loss(y_true, y_pred):\n","\n","    \"\"\"\n","    y_pred : Eucledean distance for each pair of images\n","    y_true : 1 for Genuine-genuine pair, 0 otherwise\n","    \n","    Contrastive loss from Hadsell-et-al.'06\n","    Source: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n","    \n","    Explanation:\n","    When ytrue is 1, that means the sample are duplicates of each other, \n","    so the Euclidean distance (ypred) between their outputs must be minimized.\n","    So the loss is taken as the square of that Euclidean distance itself - square(y_pred).\n","\n","    When ytrue is 0, i.e. the samples are not duplicates, then the Euclidean distance \n","    between them must be maximized, at least to the margin. So the loss to be minimized\n","    is the difference of the margin and the Euclidean distance - (margin - y_pred).\n","    If the Euclidean distance (ypred) is already greater than the margin, \n","    then nothing is to be learned, so the loss is made to be zero in \n","    that case by saying maximum(margin - y_pred, 0).\n","    \"\"\"\n","\n","    margin = 1\n","\n","    #Loss when pairs are genuine-genuine\n","    positive_loss = tf.keras.backend.square(y_pred)\n","    #Loss when pairs are genuine-fake\n","    negative_loss = tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0))\n","\n","    #Total loss\n","    total_loss = y_true * positive_loss + (1 - y_true) * negative_loss\n","    \n","    #Calculate average loss\n","    total_average_loss = tf.keras.backend.mean(total_loss)\n","\n","    return total_average_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QoXmEpF2nlgS"},"source":["Compile the model with optimizer and loss"]},{"cell_type":"code","metadata":{"id":"UXA8T8ONnnzv"},"source":["model.compile(optimizer='adam', loss=contrastive_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9BEs2VpnyEX"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zctr-X7Sn2wv"},"source":["#### Train Model"]},{"cell_type":"code","metadata":{"id":"kkA103s-oMSh"},"source":["#Total training and test examples\n","total_train_examples = len(train_g_g_pairs) + len(train_g_f_pairs)\n","total_test_examples = len(test_g_g_pairs) + len(test_g_f_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewOmXWuhrXiH"},"source":["print('Training Data:',total_train_examples)\n","print('Testing Data:',total_test_examples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnLvtqe9n1Bv"},"source":["#Create Train and Test batch generators\n","batch_size = 32\n","train_generator = batch_generator(train_g_g_pairs, train_g_f_pairs, batch_size=batch_size)\n","test_generator = batch_generator(test_g_g_pairs, test_g_f_pairs, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxwqkUi8pG_f"},"source":["#Model checkpoint to save the best model\n","model_ckpt = tf.keras.callbacks.ModelCheckpoint('anjar-border_siamese.h5', \n","                                                save_best_only=True, \n","                                                monitor='val_loss',\n","                                                verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrV-3h8FoZ_k"},"source":["#Start training\n","model.fit(train_generator,\n","          epochs=1000,\n","          steps_per_epoch=total_train_examples//batch_size, \n","          validation_data=test_generator, \n","          validation_steps=total_test_examples//batch_size, \n","          callbacks=[model_ckpt])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xXYjsMUAy7y"},"source":["#### Save Model"]},{"cell_type":"code","metadata":{"id":"Bm4dSAOs_8iS"},"source":["#Save model - change path to whatever you want\n","save_path = '/content/drive/MyDrive/siamese_face_match/face_match_siamese.h5'\n","model.save(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"keMTaG-lA3M0"},"source":["#Load model\n","# model = tf.keras.models.load_model(save_path, custom_objects={'contrastive_loss':contrastive_loss})\n","\n","model = tf.keras.models.load_model(\"/content/drive/MyDrive/siamese_face_match/face_match_siamese.h5\", custom_objects={'contrastive_loss':contrastive_loss})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxsbC7iJBwEJ"},"source":["#Make sure model has loaded\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X28jYq8NqkmF"},"source":["#### Model Accuracy"]},{"cell_type":"markdown","metadata":{"id":"QTlVj7zp7o02"},"source":["Calculate prediction for all test examples"]},{"cell_type":"code","metadata":{"id":"aYh7abc2vcuD"},"source":["#Build predictions\n","predictions = []\n","true_labels = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ho03lUb8q5S"},"source":["for i in tqdm(range(total_test_examples//batch_size)):\n","\n","    #Get batch\n","    X, y = next(test_generator)\n","    #Model predictions\n","    distances = model.predict(X)\n","\n","    #Capture it in the labels and predictions list\n","    for j in range(y.shape[0]):\n","        true_labels.append(int(y[j][0]))\n","        predictions.append(distances[j][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaSN7S47-K0B"},"source":["len(predictions), len(true_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPH3Dx2NqnpV"},"source":["How do we calculate a **threhold** above which images will be considered as same and different border type pairs pair?\n","\n","*We can check at which distance, test accuracy is highest and consider that as a threhold.*"]},{"cell_type":"code","metadata":{"id":"CZPIyYydqnCO"},"source":["def compute_accuracy_thresh(predictions, labels):\n","    \n","    \"\"\"\n","    Compute accuracy with a range of thresholds on distances.\n","    \"\"\"\n","\n","    #Get maximum and minimum value of distance for test examples\n","    dmax = np.max(predictions)\n","    dmin = np.min(predictions)\n","\n","    #How many pairs are genuine-genuine and how many are genuine-forged in test data\n","    n_gg_pairs = np.sum(labels == 1)\n","    n_gf_pairs = np.sum(labels == 0)\n","    \n","    #We will increment threhold by\n","    step = 0.01\n","\n","    #Initialize Accuracy and threshold\n","    max_acc = 0\n","    best_thresh = -1\n","\n","    #Run through a look increasing threshold by step amount and checking accuracy   \n","    for d in np.arange(dmin, dmax+step, step):\n","\n","        #Test examples for which predicted distance was less than or equal to d (threshold)\n","        #These can be taken as genuine-genuine pairs (for given threshold)\n","        idx1 = predictions.ravel() <= d\n","        \n","        #Test examples for which predicted distance > d (genuine-forged pairs)\n","        idx2 = predictions.ravel() > d\n","       \n","        #How many positive examples are correct\n","        true_positive_rate = float(np.sum(labels[idx1] == 1)) / n_gg_pairs   \n","        true_negative_rate = float(np.sum(labels[idx2] == 0)) / n_gf_pairs\n","        \n","        #Accuracy - avg of above two terms\n","        acc = (true_positive_rate + true_negative_rate)/2       \n","\n","        #If accuracy improved from previous best, make a note of it    \n","        if (acc > max_acc):\n","            max_acc, best_thresh = acc, d\n","           \n","    return max_acc, best_thresh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQSVNodYwaEB"},"source":["Calculate best threshold and accuracy"]},{"cell_type":"code","metadata":{"id":"H61woxSCuxKi"},"source":["test_acc, threshold = compute_accuracy_thresh(np.array(predictions), np.array(true_labels))\n","print('Test accuracy:', round(test_acc,2))\n","print('Best distance threshold:', round(threshold,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_4c__eXSxXNW"},"source":["#### Visualize Model Prediction"]},{"cell_type":"code","metadata":{"id":"z1bRPdtlxaw7"},"source":["def visualize_prediction(img_pairs, label):\n","\n","    #Load images\n","    first_img = tf.keras.preprocessing.image.load_img(img_pairs[0], target_size=(img_height, img_width))\n","    second_img = tf.keras.preprocessing.image.load_img(img_pairs[1], target_size=(img_height, img_width))\n","    \n","    #Convert to array\n","    first_img_array = tf.keras.preprocessing.image.img_to_array(first_img)\n","    second_img_array = tf.keras.preprocessing.image.img_to_array(second_img)\n","\n","    #Convert to a batch\n","    first_img_array = np.expand_dims(first_img_array, axis=0)\n","    second_img_array = np.expand_dims(second_img_array, axis=0)\n","\n","    #Normalize data\n","    first_img_array_norm = tf.keras.applications.mobilenet.preprocess_input(first_img_array)\n","    second_img_array_norm = tf.keras.applications.mobilenet.preprocess_input(second_img_array)\n","\n","    #Model prediction - distance\n","    distance = model.predict([first_img_array_norm, second_img_array_norm])\n","\n","    print('Actual label:', label)\n","\n","    if distance <= threshold:\n","        print('Predicted label:', 'Same')\n","    else:\n","        print('Predicted label:', 'Different')\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,20))\n","\n","    ax1.imshow(plt.imread(img_pairs[0]), cmap='gray')  \n","    ax2.imshow(plt.imread(img_pairs[1]), cmap='gray')\n","    \n","    plt.show()\n","\n","    print(distance)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHGCy1EEnWF8"},"source":["#Visualize for same border pair\n","idx = np.random.randint(0, len(test_g_g_pairs))\n","visualize_prediction(test_g_g_pairs[idx], 'Same')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXWBvimJ0ZmM"},"source":["#Visualize for different border pair\n","idx = np.random.randint(0, len(test_g_f_pairs))\n","visualize_prediction(test_g_f_pairs[idx], 'Different')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEJknluG31MK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A56dTEaEJd_V"},"source":["# For Testing Purpose"]},{"cell_type":"code","metadata":{"id":"WCBnxzbsJshI"},"source":["import tensorflow as tf\n","import cv2\n","import numpy as np \n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Xa_E7M0kXAR"},"source":["tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvQDjyIBKbFJ"},"source":["def contrastive_loss(y_true, y_pred):\n","\n","    margin = 1\n","\n","    #Loss when pairs are genuine-genuine\n","    positive_loss = tf.keras.backend.square(y_pred)\n","    #Loss when pairs are genuine-fake\n","    negative_loss = tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0))\n","\n","    #Total loss\n","    total_loss = y_true * positive_loss + (1 - y_true) * negative_loss\n","    \n","    #Calculate average loss\n","    total_average_loss = tf.keras.backend.mean(total_loss)\n","\n","    return total_average_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2g4zyB0ft5Q"},"source":["model_path = '/content/drive/MyDrive/siamese_face_match/face_match_siamese.h5'\n","\n","load_model = tf.keras.models.load_model(model_path, custom_objects={'contrastive_loss':contrastive_loss})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7nLSpmNhduq"},"source":["def border2(path1, path2):\n","\n","    # fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,20))\n","    # ax1.imshow(plt.imread(path1), cmap='gray')  \n","    # ax2.imshow(plt.imread(path2), cmap='gray')\n","    # plt.show()\n","\n","    img1 = tf.keras.preprocessing.image.load_img(path1, target_size=(300,300))\n","    img2 = tf.keras.preprocessing.image.load_img(path2, target_size=(300,300))\n","\n","    img1 = tf.keras.preprocessing.image.img_to_array(img1)\n","    img2 = tf.keras.preprocessing.image.img_to_array(img2)\n","\n","    img1 = np.expand_dims(img1, axis=0)\n","    img2 = np.expand_dims(img2, axis=0)\n","\n","    img1 = tf.keras.applications.mobilenet.preprocess_input(img1)\n","    img2 = tf.keras.applications.mobilenet.preprocess_input(img2)\n","\n","    output = [img1,img2]\n","\n","    pred = load_model.predict(output)\n","\n","    if pred[0][0] > 0.37:\n","      print(\"Prediction : Different Faces\")\n","    else:\n","      print(\"Prediction : Same Faces\")\n","    # print(pred)\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,20))\n","    ax1.imshow(plt.imread(path1), cmap='gray')  \n","    ax2.imshow(plt.imread(path2), cmap='gray')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmjeLsrnu-aX"},"source":["#give the path for images you want to test\n","\n","img1_path = '/content/drive/MyDrive/siamese_face_match/data/train/0/1.jpg'\n","img2_path = '/content/drive/MyDrive/siamese_face_match/data/train/0/2.jpg'\n","\n","z = border2(img1_path, img2_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ov-kyGlbvK0_"},"source":[""],"execution_count":null,"outputs":[]}]}